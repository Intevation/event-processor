#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Implemented workarounds:
    - postgres to_char(time.source, 'TZ') instead of 'OF' (from 9.4 up)
      This gives the timezone instead of the offset (empty if UTC)
    - BytesIO instead of StringIO on Python 2 for csv module

TODO: "feed.name" ILIKE '%' is slow
"""
import smtplib
import argparse
import csv
import datetime
import io
import json
import locale
import os
import pprint
import subprocess
import sys
import tempfile
import zipfile
import six
import prettytable
import psycopg2
import psycopg2.extras
from termstyle import bold, green, inverted, red, reset
from intelmq.lib import utils
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from email.mime.application import MIMEApplication

""" options """
dryrun = False
verbose = False
compress_csv = False
boilerplate = None

if locale.getpreferredencoding() != 'UTF-8':
    print(red('The preferred encoding of your locale setting is not UTF-8 but'
              '{}. Exiting.'.format(locale.getpreferredencoding())))
    exit(1)

myinverted = str(reset) + str(inverted)
if sys.version[0] == '2':
    input = raw_input

CONFIG = dict()
home = os.path.expanduser("~")  # needed for OSX
with open(os.path.expanduser(home + '/.intelmq/intelmqcli.conf')) as conf_handle:
    user_config = json.load(conf_handle)
with open('/etc/intelmq/intelmqcli.conf') as conf_handle:
    CONFIG = json.load(conf_handle)

for key, value in user_config.items():
    if key in CONFIG and type(CONFIG[key]) is dict:
        CONFIG[key].update(value)
    else:
        CONFIG[key] = value

EMAIL_FROM = 'noreply@example.com'

APPNAME = "intelmqcli"
DESCRIPTION = """
"""
EPILOG = """
Searches for all unprocessed incidents. Incidents will be filtered by country
code and the TLD of a domain according to configuration.
The search can be restricted to one source feed.

After the start, intelmqcli will immediately connect to RT with the given
credentials. The incidents will be shown grouped by the contact address if
known or the ASN otherwise.

You have 3 options here:
* Select one group by giving the id (number in first column) and show the email
and all events in detail
* Automatic sending of all incidents with 'a'
* Quit with 'q'

For the detailed view, the recipient, the subject and the mail text will be
shown, and below the technical data as csv. If the terminal is not big enough,
the data will not be shown in full. In this case, you can press 't' for the
table mode. less will be opened with the full text and data, whereas the data
will be formated as table, which is much easier to read and interpret.
The requestor (recipient of the mail) can be changed manually by pressing 'r'
and in the following prompt the address is asked. After sending, you can
optionally save the (new) address to the database linked to the ASNs.
If you are ready to submit the incidents to RT and send the mails out, press
's'.
'b' for back jumps to the incident overview and 'q' quits.
"""
USAGE = '''
    intelmqcli
    intelmqcli --dry-run
    intelmqcli --verbose
    intelmqcli --compress-csv
    intelmqcli --list-texts
    intelmqcli --text='boilerplate name'
    intelmqcli --feed='feedname' '''

CON_EVENTDB = psycopg2.connect(database=CONFIG['database']['event']['name'],
                               user=CONFIG['database']['event']['username'],
                               password=CONFIG['database']['event']['password'],
                               host=CONFIG['database']['event']['host'],
                               port=CONFIG['database']['event']['port'],
                               #sslmode=CONFIG['database']['event']['sslmode'],
                               )
CUR_EVENTDB = CON_EVENTDB.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
CON_EVENTDB.autocommit = True

TABLE_MODE = False  # for sticky table mode

QUERY_COUNT_ASN = """
SELECT
    COUNT(*) as count,
    COALESCE({conttab}.contacts, '') as contacts,
    string_agg(DISTINCT cast({evtab}."source.asn" as varchar), ', ') as asn,
    string_agg(DISTINCT {evtab}."classification.type", ', ') as classification,
    string_agg(DISTINCT {evtab}."feed.code", ', ') as feeds,
    COALESCE({conttab}.contacts, cast({evtab}."source.asn" as varchar))
        as grouping
FROM {evtab}
LEFT OUTER JOIN as_contacts ON {evtab}."source.asn" = {conttab}.asnum
WHERE
    notify = TRUE AND (
        {evtab}.rtir_report_id IS NULL OR
        {evtab}.rtir_incident_id IS NULL OR
        {evtab}.rtir_investigation_id IS NULL
    )
    AND
    (
        {evtab}."source.geolocation.cc" LIKE '{cc}' OR
        {evtab}."source.fqdn" LIKE %s
    )
    AND "feed.name" ILIKE %s AND
    "time.source" IS NOT NULL
GROUP BY {conttab}.contacts, grouping;
"""
###.format(evtab=CONFIG['database']['events_table'], cc=CONFIG['filter']['cc'],
###           conttab=CONFIG['database']['contacts_table'])


""" This is the list of fields (and their respective order) which we intend to
send out.  This is based on the order and fields of shadowserver.

Shadowserver format:
    timestamp,"ip","protocol","port","hostname","packets","size","asn","geo","region","city",
    "naics","sic","sector"
"""

CSV_FIELDS = ["time.source", "source.ip", "protocol.transport", "source.port",
              "protocol.application",
              "source.fqdn", "source.local_hostname", "source.local_ip", "source.url",
              "source.asn", "source.geolocation.cc",
              "source.geolocation.city",
              "classification.taxonomy", "classification.type", "classification.identifier",
              "destination.ip", "destination.port", "destination.fqdn", "destination.url",
              "feed", "event_description.text", "event_description.url", "malware.name", "comment",
              "additional_field_freetext", "version: 1.0"
              ]

QUERY_GET_EVENTS = """SELECT * FROM events LIMIT 40"""

QUERY_BY_ASCONTACT = """
SELECT
    to_char({evtab}."time.source",
            'YYYY-MM-DD"T"HH24:MI:SSOF') as "time.source",
    {evtab}.id,
    {evtab}."feed.code" as feed,
    {evtab}."source.ip",
    {evtab}."source.port",
    {evtab}."source.asn",
    {evtab}."source.network",
    {evtab}."source.geolocation.cc",
    {evtab}."source.geolocation.region",
    {evtab}."source.geolocation.city",
    {evtab}."source.account",
    {evtab}."source.fqdn",
    {evtab}."source.local_hostname",
    {evtab}."source.local_ip",
    {evtab}."source.reverse_dns",
    {evtab}."source.tor_node",
    {evtab}."source.url",
    {evtab}."classification.identifier",
    {evtab}."classification.taxonomy",
    {evtab}."classification.type",
    {evtab}."comment",
    {evtab}."destination.ip",
    {evtab}."destination.port",
    {evtab}."destination.asn",
    {evtab}."destination.network",
    {evtab}."destination.geolocation.cc",
    {evtab}."destination.geolocation.region",
    {evtab}."destination.geolocation.city",
    {evtab}."destination.account",
    {evtab}."destination.fqdn",
    {evtab}."destination.local_hostname",
    {evtab}."destination.local_ip",
    {evtab}."destination.reverse_dns",
    {evtab}."destination.tor_node",
    {evtab}."destination.url",
    {evtab}."event_description.target",
    {evtab}."event_description.text",
    {evtab}."event_description.url",
    {evtab}."event_hash",
    {evtab}."extra",
    {evtab}."feed.accuracy",
    {evtab}."malware.hash",
    {evtab}."malware.hash.md5",
    {evtab}."malware.hash.sha1",
    {evtab}."malware.name",
    {evtab}."malware.version",
    {evtab}."misp_uuid",
    {evtab}."notify",
    {evtab}."protocol.application",
    {evtab}."protocol.transport",
    {evtab}."screenshot_url",
    {evtab}."status",
    {evtab}."time.observation"
FROM events
LEFT OUTER JOIN {conttab} ON {evtab}."source.asn" = {conttab}.asnum
WHERE
    notify = TRUE AND (
        {evtab}.rtir_report_id IS NULL OR
        {evtab}.rtir_incident_id IS NULL OR
        {evtab}.rtir_investigation_id IS NULL
    ) AND
    (
        {evtab}."source.geolocation.cc" LIKE '{cc}' OR
        {evtab}."source.fqdn" LIKE %s
    ) AND
    {conttab}.contacts = %s AND
    "feed.name" ILIKE %s AND
    "time.source" IS NOT NULL;
"""
###""".format(evtab=CONFIG['database']['events_table'], cc=CONFIG['filter']['cc'],
###           conttab=CONFIG['database']['contacts_table'])


QUERY_BY_ASNUM = """
SELECT
    to_char({evtab}."time.source" at time zone 'UTC',
            'YYYY-MM-DD"T"HH24:MI:SSOF') as "time.source",
    {evtab}.id,
    {evtab}."feed.code" as feed,
    {evtab}."source.ip",
    {evtab}."source.port",
    {evtab}."source.asn",
    {evtab}."source.network",
    {evtab}."source.geolocation.cc",
    {evtab}."source.geolocation.region",
    {evtab}."source.geolocation.city",
    {evtab}."source.account",
    {evtab}."source.fqdn",
    {evtab}."source.local_hostname",
    {evtab}."source.local_ip",
    {evtab}."source.reverse_dns",
    {evtab}."source.tor_node",
    {evtab}."source.url",
    {evtab}."classification.identifier",
    {evtab}."classification.taxonomy",
    {evtab}."classification.type",
    {evtab}."comment",
    {evtab}."destination.ip",
    {evtab}."destination.port",
    {evtab}."destination.asn",
    {evtab}."destination.network",
    {evtab}."destination.geolocation.cc",
    {evtab}."destination.geolocation.region",
    {evtab}."destination.geolocation.city",
    {evtab}."destination.account",
    {evtab}."destination.fqdn",
    {evtab}."destination.local_hostname",
    {evtab}."destination.local_ip",
    {evtab}."destination.reverse_dns",
    {evtab}."destination.tor_node",
    {evtab}."destination.url",
    {evtab}."event_description.target",
    {evtab}."event_description.text",
    {evtab}."event_description.url",
    {evtab}."event_hash",
    {evtab}."extra",
    {evtab}."feed.accuracy",
    {evtab}."malware.hash",
    {evtab}."malware.hash.md5",
    {evtab}."malware.hash.sha1",
    {evtab}."malware.name",
    {evtab}."malware.version",
    {evtab}."misp_uuid",
    {evtab}."notify",
    {evtab}."protocol.application",
    {evtab}."protocol.transport",
    {evtab}."screenshot_url",
    {evtab}."status",
    {evtab}."time.observation"
FROM events
LEFT OUTER JOIN {conttab} ON {evtab}."source.asn" = {conttab}.asnum
WHERE
    notify = TRUE AND (
        {evtab}.rtir_report_id IS NULL OR
        {evtab}.rtir_incident_id IS NULL OR
        {evtab}.rtir_investigation_id IS NULL
    ) AND
    (
        {evtab}."source.geolocation.cc" LIKE '{cc}' OR
        {evtab}."source.fqdn" LIKE %s
    ) AND
    {evtab}."source.asn" = %s AND
    "feed.name" ILIKE %s AND
    "time.source" IS NOT NULL;
"""
###""".format(evtab=CONFIG['database']['events_table'], cc=CONFIG['filter']['cc'],
###           conttab=CONFIG['database']['contacts_table'])



QUERY_GET_TEXT = """
SELECT
    body
FROM {texttab}
WHERE
    key = %s
"""
#""".format(texttab=CONFIG['database']['text_table'])


QUERY_TEXT_NAMES = "SELECT DISTINCT \"key\" from boilerplates"


def get_text(query):
    types = [row['classification.identifier'] for row in query
             if 'classification.identifier' in row]
    text = None
    if boilerplate:
        text_id = boilerplate
    else:
        text_id = types[0]
    if len(types) == 1:
        CUR.execute(QUERY_GET_TEXT, (text_id,))
        if CUR.rowcount:
            text = CUR.fetchall()[0]['body']

    if text is None and boilerplate:
        return red('--text param given, but boilerplate text %s not found!' % text_id)

    if text is None:
        CUR.execute(QUERY_GET_TEXT, (CONFIG['database']['default_key'],))
        if CUR.rowcount:
            text = CUR.fetchall()[0]['body']
        else:
            return red('Default text not found!')

    return text


def target_from_row(row):
    """
    Returns the first value in give row that exists from this list of keys:
    'source.ip', 'source.fqdn', 'source.url', 'source.account'
    """
    keys = ['source.ip', 'source.fqdn', 'source.url', 'source.account']
    for key in keys:
        if key in row:
            return row[key]


def shrink_dict(d):
    if not compress_csv:
        return d
    keys = d[0].keys()
    empty = dict(zip(keys, [True] * len(keys)))
    for line in d:
        for key, value in line.items():
            if value is not None:
                empty[key] = False
    return [{k: v for k, v in dicti.items() if not empty[k]} for dicti in d]


def getTerminalHeight():
    return int(subprocess.check_output(['stty', 'size']).strip().split()[0])


def count_by_asn(feed='%'):
    # TODO: Existing RT ids!
    CUR.execute(QUERY_COUNT_ASN, (CONFIG['filter']['fqdn'], feed))
    asn_count = CUR.fetchall()
    if not asn_count:
        print('No incidents!')
        exit(0)
    print('=' * 100)
    table = prettytable.PrettyTable(map(bold, ['id', 'n°', 'ASNs', 'contacts',
                                               'types', 'feeds']))
    table.align = 'l'
    for number, row in enumerate(asn_count):
        table.add_row([number, row['count'], row['asn'], row['contacts'],
                       row['classification'], row['feeds']])
    print(table.get_string(border=False, padding_width=1))

    print('{} incidents for {} contacts.'
          ''.format(sum((row['count'] for row in asn_count)), len(asn_count)))
    return asn_count


def send_email(event):
    msg = MIMEMultipart(
        From=EMAIL_FROM,
        To=event['email_to'],
        Subject=event['subject']
    )
    msg.attach(MIMEText(event['email_body']))

    for f in event['files'] or []:
        with open(f, "rb") as fil:
            msg.attach(MIMEApplication(
                fil.read(),
                Content_Disposition='attachment; filename="%s"' % os.path.basename(f),
                Name=basename(f)
            ))

    smtp = smtplib.SMTP(server)
    smtp.sendmail(EMAIL_FROM, event['email_to'], msg.as_string())
    smtp.close()

def process_events(event_list):
    for event in event_list:
        send_email(event)
    pass

def get_event_list():
    '''
    Retrieves the current event list
    '''
    CUR_EVENTDB.execute(QUERY_GET_EVENTS)
    event_list = [i for i in CUR_EVENTDB.fetchall()]

    return event_list


if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        prog=APPNAME,
        formatter_class=argparse.RawDescriptionHelpFormatter,
        usage=USAGE,
        description=DESCRIPTION,
        epilog=EPILOG,
    )
    parser.add_argument('-L', '--list-texts', action='store_true',
                        help='List all existing texts.')
    parser.add_argument('-t', '--text', nargs=1, help='Specify the text to be used.')
    parser.add_argument('-a', '--all', action='store_true', help='Process all events (batch mode)')
    parser.add_argument('-f', '--feed', nargs='?', default='%', const='%',
                        help='Show only incidents reported by given feed.')
    parser.add_argument('-v', '--verbose', action='store_true',
                        help='Print verbose messages.')
    parser.add_argument('-c', '--compress-csv', action='store_true',
                        help='Automatically compress/shrink the attached CSV report if fields are '
                             'empty (default = False).')
    parser.add_argument('-n', '--dry-run', action='store_true',
                        help='Do not store anything or change anything. Just simulate.')
    args = parser.parse_args()

    if args.verbose:
        verbose = True
    if args.dry_run:
        dryrun = True
    if args.compress_csv:
        compress_csv = True
    if args.text:
        boilerplate = args.text

    event_list = get_event_list()
    while event_list:
        to_process = event_list[:10]
        if not args.all:
            print('Current batch:')
            for i in to_process:
                print('    * {0}'.format(i.get('event_description.text')))
       
        if args.all:
            answer = 'a'
        else:
            answer = input('Options: [c]ontinue, [s]end this batch, send [a]ll, [q]uit? ')

        if answer == 'c':
            print('Continue')
        elif answer == 's':
            print('Sending this batch')
            event_list[:10] = []
            process_events(to_process)
        elif answer == 'a':
            print('Sending all')
            to_process = event_list[:]
            event_list = []
            process_events(to_process)
        else:
            print('Exiting')
            break

    if args.list_texts:
        CUR.execute(QUERY_TEXT_NAMES)
        for row in CUR.fetchall():
            if row['key']:
                print(row['key'])
        exit(0)

    # try:
    #     while True:
    #         asn_count = count_by_asn(feed=args.feed)
    #         if verbose:
    #             print(sys.stderr, 'asn_count = {}.'.format(asn_count))
    #         answer = input('{i}detailed view by id, {b}[a]{i}utomatic '
    #                        'sending, {b}[q]{i}uit?{r} '
    #                        ''.format(b=bold, i=myinverted, r=reset)).strip()
    #         try:
    #             answer = int(answer)
    #         except ValueError:
    #             pass
    #         if answer == 'q':
    #             break
    #         elif answer == 'a':
    #             for item in asn_count:
    #                 if item['contacts']:
    #                     query_by_as(item['contacts'], automatic=True,
    #                                 feed=args.feed)
    #                 else:
    #                     if item['asn']:
    #                         query_by_as(int(item['asn']), automatic=True,
    #                                     feed=args.feed)
    #                     else:
    #                         print(red('Can not query the data of an unknown ASN. Ignoring.'))
    #         elif type(answer) is int:
    #             if asn_count[answer]['contacts']:
    #                 query_by_as(asn_count[answer]['contacts'], feed=args.feed)
    #             else:
    #                 if asn_count[answer] and asn_count[answer]['asn']:
    #                     query_by_as(int(asn_count[answer]['asn']), feed=args.feed)
    #                 else:
    #                     print(red('no ASNs known. Ignoring.'))
    #         else:
    #             print(red('Unknown answer {!r}.'.format(answer)))
    #
    # except BaseException as exc:
    #     if isinstance(exc, (SystemExit, KeyboardInterrupt)):
    #         print()
    #     else:
    #         raise
    #
